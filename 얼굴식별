import cv2
import numpy as np
import torch
from facenet_pytorch import MTCNN, InceptionResnetV1
from scipy.spatial.distance import cosine

# MTCNN 모델 로드 (얼굴 검출)
mtcnn = MTCNN(keep_all=True)

# InceptionResnetV1 모델 로드 (FaceNet)
model = InceptionResnetV1(pretrained='vggface2').eval()

# npz 파일에서 알려진 얼굴 임베딩 및 레이블 로드
data = np.load("C:/Projects/Python/trainEmbeds.npz")
known_embeddings = data['x']
labels = data['y']

# 얼굴 임베딩 함수
def get_embedding(model, face):
    face = face.unsqueeze(0)
    with torch.no_grad():
        embedding = model(face)
    return embedding[0].cpu().numpy()

# 웹캠 초기화
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    boxes, _ = mtcnn.detect(frame_rgb)

    if boxes is not None:
        for box in boxes:
            x1, y1, x2, y2 = map(int, box)
            face = frame_rgb[y1:y2, x1:x2]
            if face.size == 0:
                continue  # 얼굴 영역이 비어있으면 건너뜁니다.
            face = cv2.resize(face, (160, 160))
            face = torch.tensor(face).permute(2, 0, 1).float() / 255.0

            embedding = get_embedding(model, face)

            min_dist = float('inf')
            label = 'Unknown'
            for i, known_embedding in enumerate(known_embeddings):
                dist = cosine(embedding, known_embedding)
                if dist < min_dist:
                    min_dist = dist
                    label = labels[i]

            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(frame, str(label), (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    cv2.imshow('Face Recognition', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
